{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stopwords in English:\n",
      "{\"hasn't\", 'wasn', 'while', 'them', 'shouldn', 'just', 'haven', 'most', 'above', \"should've\", \"shouldn't\", \"wouldn't\", 'you', 'am', 'any', 'than', 'few', 'o', 'ma', 'why', 'll', 'these', 'to', 'by', 'ain', \"that'll\", 'or', 'will', 'at', 'shan', 'what', \"isn't\", 'mustn', \"won't\", 'and', 'down', \"mightn't\", 'with', \"she's\", 'they', 'being', 'does', 'an', 'same', 'y', \"doesn't\", 'their', 'weren', 'your', 'hadn', 'whom', 'we', 'no', 're', 'as', \"you'll\", 'not', 'yourself', 'such', \"weren't\", \"couldn't\", 'until', 'me', 'themselves', 'be', 'but', 'didn', 'i', 'his', 'where', 'now', 'here', 'is', 'm', 'wouldn', 'which', 'nor', 's', 'again', 'are', 'yours', 'did', 'once', 'some', \"it's\", 'when', 'because', \"needn't\", 'mightn', \"you'd\", 'ours', 'ourselves', 'there', \"wasn't\", 'so', 'her', 'very', 'won', 'from', 'through', 'further', 'other', \"aren't\", 'then', 'been', 'both', 'having', 'do', 'about', 'don', \"hadn't\", 'a', 'needn', 'theirs', 'who', 'has', 'off', 'was', 'those', 'during', 'before', 'own', 'itself', 'he', 'this', \"shan't\", 'doesn', 'if', 'in', 'of', 'how', 'against', 'him', 'between', 'd', \"mustn't\", 'aren', 'too', 'can', \"you've\", 'isn', \"didn't\", 'for', 'doing', 'that', 'into', 'under', 'myself', 'out', 't', 'hasn', 'more', 'our', 'after', 'up', \"you're\", 'couldn', 'have', 'each', 'yourselves', 've', 'my', 'hers', 'herself', 'himself', 'should', \"haven't\", 'the', 'only', 'had', 'it', 'below', 'were', 'all', 'she', 'over', \"don't\", 'its', 'on'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "result = set(stopwords.words('english'))\n",
    "print(\"List of stopwords in English:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After removing stop words from the said text:\n",
      "['In', 'computing,', 'stop', 'words', 'words', 'filtered', 'processing', 'natural', 'language', 'data', '(text).', 'Though', '\"stop', 'words\"', 'usually', 'refers', 'common', 'words', 'language,', 'single', 'universal', 'list', 'stop', 'words', 'used', 'natural', 'language', 'processing', 'tools,', 'indeed', 'tools', 'even', 'use', 'list.', 'Some', 'tools', 'specifically', 'avoid', 'removing', 'stop', 'words', 'support', 'phrase', 'search.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "text = '''\n",
    "In computing, stop words are words which are filtered out before or after \n",
    "processing of natural language data (text). Though \"stop words\" usually \n",
    "refers to the most common words in a language, there is no single universal \n",
    "list of stop words used by all natural language processing tools, and \n",
    "indeed not all tools even use such a list. Some tools specifically avoid \n",
    "removing these stop words to support phrase search.\n",
    "'''\n",
    "clean_word_list = [word for word in text.split() if word not in stoplist]\n",
    "print(\"\\nAfter removing stop words from the said text:\")\n",
    "print(clean_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of fresh stopwords in English:\n",
      "{\"hasn't\", 'wasn', 'while', 'them', 'shouldn', 'just', 'haven', 'most', 'above', \"should've\", \"shouldn't\", \"wouldn't\", 'you', 'am', 'any', 'than', 'few', 'o', 'ma', 'why', 'll', 'these', 'to', 'by', 'ain', \"that'll\", 'or', 'will', 'at', 'shan', 'what', \"isn't\", 'mustn', \"won't\", 'and', 'down', \"mightn't\", 'with', \"she's\", 'they', 'being', 'does', 'an', 'same', 'y', \"doesn't\", 'their', 'weren', 'your', 'hadn', 'whom', 'we', 'no', 're', 'as', \"you'll\", 'not', 'yourself', 'such', \"weren't\", \"couldn't\", 'until', 'me', 'themselves', 'be', 'but', 'didn', 'i', 'his', 'where', 'now', 'here', 'is', 'm', 'wouldn', 'which', 'nor', 's', 'are', 'yours', 'did', 'some', \"it's\", 'when', 'because', \"needn't\", 'mightn', \"you'd\", 'ours', 'ourselves', 'there', \"wasn't\", 'so', 'her', 'very', 'won', 'through', 'further', 'other', \"aren't\", 'then', 'been', 'both', 'having', 'do', 'about', 'don', \"hadn't\", 'a', 'needn', 'theirs', 'who', 'has', 'off', 'was', 'those', 'during', 'before', 'own', 'itself', 'he', 'this', \"shan't\", 'doesn', 'if', 'in', 'of', 'how', 'against', 'him', 'between', 'd', \"mustn't\", 'aren', 'too', 'can', \"you've\", 'isn', \"didn't\", 'for', 'doing', 'that', 'into', 'under', 'myself', 'out', 't', 'hasn', 'more', 'our', 'after', 'up', \"you're\", 'couldn', 'have', 'each', 'yourselves', 've', 'my', 'hers', 'herself', 'himself', 'should', \"haven't\", 'the', 'only', 'had', 'it', 'below', 'were', 'all', 'she', 'over', \"don't\", 'its', 'on'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english')) - set(['again', 'once', 'from'])\n",
    "print(\"\\nList of fresh stopwords in English:\")\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: WordNet in c:\\users\\win10\\anaconda3\\lib\\site-packages (0.0.1b2)\n",
      "Requirement already satisfied: colorama==0.3.9 in c:\\users\\win10\\anaconda3\\lib\\site-packages (from WordNet) (0.3.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defination of the said word:\n",
      "a hostile meeting of opposing military forces in the course of a war\n",
      "\n",
      "Examples of the word in use::\n",
      "['Grant won a decisive victory in the battle of Chickamauga', 'he lost his romantic ideas about war when he got into a real engagement']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "syns = wordnet.synsets(\"fight\")\n",
    "print(\"Defination of the said word:\")\n",
    "print(syns[0].definition())\n",
    "print(\"\\nExamples of the word in use::\")\n",
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of words:\n",
      "['Joe', 'waited', 'for', 'the', 'train', '.', 'The', 'train', 'was', 'late', '.', 'Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.', 'I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'station', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\"\n",
    "print(\"\\nList of words:\")\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "\n",
      "Joe waited for the train. The train was late. \n",
      "Mary and Samantha took the bus. \n",
      "I looked for Mary and Samantha at the bus station.\n",
      "\n",
      "\n",
      "Sentence-tokenized copy in a list:\n",
      "['\\nJoe waited for the train.', 'The train was late.', 'Mary and Samantha took the bus.', 'I looked for Mary and Samantha at the bus station.']\n",
      "\n",
      "Read the list:\n",
      "\n",
      "Joe waited for the train.\n",
      "The train was late.\n",
      "Mary and Samantha took the bus.\n",
      "I looked for Mary and Samantha at the bus station.\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Joe waited for the train. The train was late. \n",
    "Mary and Samantha took the bus. \n",
    "I looked for Mary and Samantha at the bus station.\n",
    "'''\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "token_text = sent_tokenize(text)\n",
    "print(\"\\nSentence-tokenized copy in a list:\")\n",
    "print(token_text)\n",
    "print(\"\\nRead the list:\")\n",
    "for s in token_text:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Reset your password if you just can't remember your old one.\n",
      "\n",
      "Split all punctuation into separate tokens:\n",
      "['Reset', 'your', 'password', 'if', 'you', 'just', 'can', \"'\", 't', 'remember', 'your', 'old', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "text = \"Reset your password if you just can't remember your old one.\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "result = WordPunctTokenizer().tokenize(text)\n",
    "print(\"\\nSplit all punctuation into separate tokens:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\n",
      "\n",
      "Tokenize words sentence wise:\n",
      "\n",
      "Read the list:\n",
      "['Joe', 'waited', 'for', 'the', 'train', '.']\n",
      "['The', 'train', 'was', 'late', '.']\n",
      "['Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.']\n",
      "['I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'station', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = \"Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\"\n",
    "print(\"\\nOriginal string:\")\n",
    "print(text)\n",
    "print(\"\\nTokenize words sentence wise:\")\n",
    "result = [word_tokenize(t) for t in sent_tokenize(text)]\n",
    "print(\"\\nRead the list:\")\n",
    "for s in result:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet:\n",
      "\n",
      "Mr. Smith waited for the train. The train was late.\n",
      "Mary and Samantha took the bus. I looked for Mary and\n",
      "Samantha at the bus station.\n",
      "\n",
      "Mr. Smith waited for the train.\n",
      "==============\n",
      "The train was late.\n",
      "==============\n",
      "Mary and Samantha took the bus.\n",
      "==============\n",
      "I looked for Mary and\n",
      "Samantha at the bus station.\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "text = '''\n",
    "Mr. Smith waited for the train. The train was late.\n",
    "Mary and Samantha took the bus. I looked for Mary and\n",
    "Samantha at the bus station.\n",
    "'''\n",
    "print(\"\\nOriginal Tweet:\")\n",
    "print(text)\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "print('\\n==============\\n'.join(sent_detector.tokenize(text.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set of synonyms of the said word:\n",
      "{'terminate', 'closing', 'terminal', 'last', 'goal', 'conclusion', 'remainder', 'ending', 'close', 'finish', 'cease', 'oddment', 'death', 'remnant', 'end', 'destruction', 'stop', 'final_stage'}\n",
      "\n",
      "Set of antonyms of the said word:\n",
      "{'beginning', 'begin'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"end\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(\"\\nSet of synonyms of the said word:\")\n",
    "print(set(synonyms))\n",
    "print(\"\\nSet of antonyms of the said word:\")\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
