{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAayJMwxMk3k"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "JWXZ53D5MnRR",
    "outputId": "fabc7d60-7abc-401e-c67c-1e12d4b59088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textsearch\n",
      "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
      "Collecting pyahocorasick\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
      "Collecting Unidecode\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "Building wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py): started\n",
      "  Building wheel for pyahocorasick (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pyahocorasick\n",
      "Failed to build pyahocorasick\n",
      "Installing collected packages: pyahocorasick, Unidecode, textsearch\n",
      "    Running setup.py install for pyahocorasick: started\n",
      "    Running setup.py install for pyahocorasick: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'c:\\users\\win10\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qepe11yj\\\\pyahocorasick\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qepe11yj\\\\pyahocorasick\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\win10\\AppData\\Local\\Temp\\pip-wheel-0cg91kfy' --python-tag cp37\n",
      "       cwd: C:\\Users\\win10\\AppData\\Local\\Temp\\pip-install-qepe11yj\\pyahocorasick\\\n",
      "  Complete output (5 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'ahocorasick' extension\n",
      "  error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pyahocorasick\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'c:\\users\\win10\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qepe11yj\\\\pyahocorasick\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qepe11yj\\\\pyahocorasick\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\win10\\AppData\\Local\\Temp\\pip-record-9xoyxqdc\\install-record.txt' --single-version-externally-managed --compile\n",
      "         cwd: C:\\Users\\win10\\AppData\\Local\\Temp\\pip-install-qepe11yj\\pyahocorasick\\\n",
      "    Complete output (5 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_ext\n",
      "    building 'ahocorasick' extension\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'c:\\users\\win10\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qepe11yj\\\\pyahocorasick\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qepe11yj\\\\pyahocorasick\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\win10\\AppData\\Local\\Temp\\pip-record-9xoyxqdc\\install-record.txt' --single-version-externally-managed --compile Check the logs for full command output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading https://files.pythonhosted.org/packages/5a/6a/7bde208d21b4f7db1b666739dca8826d8fd7af34a14fddfda9b597ab8a45/contractions-0.0.23-py2.py3-none-any.whl\n",
      "Collecting textsearch\n",
      "  Using cached https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
      "Collecting pyahocorasick\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz\n",
      "Collecting Unidecode\n",
      "  Using cached https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py): started\n",
      "  Building wheel for pyahocorasick (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pyahocorasick\n",
      "Failed to build pyahocorasick\n",
      "Installing collected packages: pyahocorasick, Unidecode, textsearch, contractions\n",
      "    Running setup.py install for pyahocorasick: started\n",
      "    Running setup.py install for pyahocorasick: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'c:\\users\\win10\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-p2381ihh\\\\pyahocorasick\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-p2381ihh\\\\pyahocorasick\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\win10\\AppData\\Local\\Temp\\pip-wheel-tbrau2eu' --python-tag cp37\n",
      "       cwd: C:\\Users\\win10\\AppData\\Local\\Temp\\pip-install-p2381ihh\\pyahocorasick\\\n",
      "  Complete output (5 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'ahocorasick' extension\n",
      "  error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for pyahocorasick\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'c:\\users\\win10\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-p2381ihh\\\\pyahocorasick\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-p2381ihh\\\\pyahocorasick\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\win10\\AppData\\Local\\Temp\\pip-record-mjsku3ig\\install-record.txt' --single-version-externally-managed --compile\n",
      "         cwd: C:\\Users\\win10\\AppData\\Local\\Temp\\pip-install-p2381ihh\\pyahocorasick\\\n",
      "    Complete output (5 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_ext\n",
      "    building 'ahocorasick' extension\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'c:\\users\\win10\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-p2381ihh\\\\pyahocorasick\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\win10\\\\AppData\\\\Local\\\\Temp\\\\pip-install-p2381ihh\\\\pyahocorasick\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\win10\\AppData\\Local\\Temp\\pip-record-mjsku3ig\\install-record.txt' --single-version-externally-managed --compile Check the logs for full command output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\win10\\anaconda3\\lib\\site-packages (4.31.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\win10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\win10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install textsearch\n",
    "!pip install contractions\n",
    "!pip install tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\Anaconda3\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in c:\\users\\ln2\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "[x] Couldn't link model to 'en'\n",
      "Creating a symlink in spacy/data failed. Make sure you have the required\n",
      "permissions and try re-running the command as admin, or use a virtualenv. You\n",
      "can still import the model as a module and call its load() method, or create the\n",
      "symlink manually.\n",
      "C:\\Users\\ln2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\en_core_web_sm\n",
      "-->\n",
      "C:\\Users\\ln2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\spacy\\data\\en\n",
      "[!] Download successful but linking failed\n",
      "Creating a shortcut link for 'en' didn't work (maybe you don't have admin\n",
      "permissions?), but you can still load the model via its full package name: nlp =\n",
      "spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You do not have sufficient privilege to perform this operation.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0G_9oc6ICdU"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import en_core_web_sm\n",
    "ps =PorterStemmer()\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def simple_stemming(text, stemmer=ps):\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
    "    if not stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    \n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Aloz8FEcICdY",
    "outputId": "7ec681c7-aff6-4237-97df-20cf7cec1640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "how are you doing\n",
      "I\tam\tdoing\tgreat\n",
      ":)\n"
     ]
    }
   ],
   "source": [
    "s = 'hello\\r\\nhow are you doing\\r\\nI\\tam\\tdoing\\tgreat\\r\\n:)'\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9lvBAtV9ICdb",
    "outputId": "176f21c8-5e8f-46a5-fc21-fa548c5e3592"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello  how are you doing  I am doing great  :)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.translate(s.maketrans(\"\\n\\t\\r\", \"   \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game is on '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "import re\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "remove_emoji(\"game is on 🔥🔥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: emot in c:\\users\\ln2\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install emot --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emo_unicode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement emo_unicode (from versions: )\n",
      "No matching distribution found for emo_unicode\n"
     ]
    }
   ],
   "source": [
    "!pip install emo_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'game is on  fire  fire '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "def convert_emojis(text):\n",
    "    return emoji.demojize(text).replace(\":\",\" \").replace(\",\",\" \")\n",
    "\n",
    "\n",
    "convert_emojis(\"game is on 🔥🔥\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"slang.txt\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its Abbreviation in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    return (' '.join(user_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = translator(\"one minute BRB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one minute Be Right Back'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34cnm0OkICdd"
   },
   "source": [
    "## Your Turn: Add in all the necessary functions and build your pre-processor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_N5OiA07ICdd"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def text_pre_processor(text, html_strip=True, accented_char_removal=True, contraction_expansion=True,\n",
    "                       text_lower_case=True, text_stemming=False, text_lemmatization=True, \n",
    "                       special_char_removal=True, remove_digits=True, stopword_removal=True, \n",
    "                       stopword_list=None,emoji_remove = False, emoticons_remove=False,emoticons_convert=False,\n",
    "                       translator=False,convert_emojis=False):\n",
    "    \n",
    "    if html_strip:\n",
    "        text = \n",
    "    return text\n",
    "\n",
    "  \n",
    "def corpus_pre_processor(corpus):\n",
    "  norm_corpus = []\n",
    "  for doc in tqdm.tqdm(corpus):\n",
    "    norm_corpus.append(text_pre_processor(doc))\n",
    "  return norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBE3P53iICdg"
   },
   "source": [
    "# Test on a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "CjL4Mx9aICdg",
    "outputId": "0525fb8c-914e-4537-a7c1-97173266708b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\n",
      " \n",
      "              It's an amazing language which can be used for [Scripting\tWeb development\tBackend development],\n",
      "\n",
      "\n",
      "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\n",
      "\n",
      "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\n",
      "\n",
      "\n",
      "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
      "           \n"
     ]
    }
   ],
   "source": [
    "document = \"\"\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
    "              It's an amazing language which can be used for [Scripting\\tWeb development\\tBackend development],\\r\\n\\r\\n\n",
    "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
    "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
    "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
    "           \"\"\"\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "q67MCFxLICdj",
    "outputId": "84033398-3704-4f6a-bfb2-edd3a34b0299"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pre_processor(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsIpekwoICdl"
   },
   "source": [
    "# Test on a corpus of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2S1H1zakICdo"
   },
   "outputs": [],
   "source": [
    "corpus = [\"\"\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
    "              It's an amazing language which can be used for [Scripting\\tWeb development\\tBackend development],\\r\\n\\r\\n\n",
    "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
    "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
    "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
    "           \"\"\",\n",
    "          \"\"\"US unveils world's most powerful supercomputer, beats China. \n",
    "             The US has unveiled the world's most powerful supercomputer \n",
    "             called 'Summit', beating the previous record-holder China's Sunway \n",
    "             TaihuLight. With a peak performance of 200,000 trillion calculations \n",
    "             per second, it is over twice as fast as Sunway TaihuLight, which is capable \n",
    "             of 93,000 trillion calculations per second. Summit has 4,608 servers, \n",
    "             which reportedly take up the size of two tennis courts.\"\"\",\n",
    "          \"\"\"The Lord of the Rings is an epic high fantasy novel written by English author and scholar J. R. R. Tolkien. \n",
    "            The story began as a sequel to Tolkien's 1937 fantasy novel The Hobbit, but eventually developed into \n",
    "            a much larger work. Written in stages between 1937 and 1949, The Lord of the Rings is one of the \n",
    "            best-selling novels ever written, with over 150 million copies sold.[1]\n",
    "          \"\"\",\n",
    "          \"\"\"The title of the novel refers to the story's main antagonist, the Dark Lord Sauron,[a] \n",
    "             who had in an earlier age created the One Ring to rule the other Rings of Power as the ultimate weapon \n",
    "             in his campaign to conquer and rule all of Middle-earth. From quiet beginnings in the Shire, a hobbit \n",
    "             land not unlike the English countryside, the story ranges across Middle-earth, following the course \n",
    "             of the War of the Ring through the eyes of its characters, not only the hobbits Frodo Baggins, \n",
    "             Samwise \"Sam\" Gamgee, Meriadoc \"Merry\" Brandybuck and Peregrin \"Pippin\" Took, but also the hobbits' \n",
    "             chief allies and travelling companions: the Men, Aragorn, a Ranger of the North, and Boromir, \n",
    "             a Captain of Gondor; Gimli, a Dwarf warrior; Legolas Greenleaf, an Elven prince; and Gandalf, a wizard.\n",
    "          \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "rj7oRS8hQYco",
    "outputId": "da2bc31f-b14f-4c2a-edac-6ee6c005faa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 19.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python',\n",
       " 'us unveil world powerful supercomputer beat china us unveil world powerful supercomputer call summit beat previous record holder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_docs = corpus_pre_processor(corpus)\n",
    "norm_docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jZRQDfTTQpzh"
   },
   "source": [
    "# Processor with multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyYMPuirQbj6"
   },
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "import threading\n",
    "\n",
    "def parallel_preprocessing(idx, doc, total_docs):\n",
    "    return text_pre_processor(doc)\n",
    "\n",
    "\n",
    "def pre_process_documents_parallel(documents):\n",
    "    total_docs = len(documents)\n",
    "    docs_input = [[idx, doc, total_docs] for idx, doc in enumerate(documents)]\n",
    "    \n",
    "    ex = futures.ThreadPoolExecutor(max_workers=None)\n",
    "    print('preprocessing: starting')\n",
    "    norm_descriptions_map = ex.map(parallel_preprocessing, \n",
    "                                   [record[0] for record in docs_input],\n",
    "                                   [record[1] for record in docs_input],\n",
    "                                   [record[2] for record in docs_input])\n",
    "    print(\"preprocessing: end\")\n",
    "    norm_descriptions = list(norm_descriptions_map)\n",
    "    return norm_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "HFBm2MmZRck3",
    "outputId": "fd917fea-0e31-46e5-af2d-2ccbea81a7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: starting\n",
      "preprocessing: end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python',\n",
       " 'us unveil world powerful supercomputer beat china us unveil world powerful supercomputer call summit beat previous record holder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court',\n",
       " 'lord rings epic high fantasy novel write english author scholar j r r tolkien story begin sequel tolkien fantasy novel hobbit eventually develop much large work write stage lord rings one best sell novel ever write million copy sold',\n",
       " 'title novel refer story main antagonist dark lord saurona early age create one ring rule rings power ultimate weapon campaign conquer rule middle earth quiet beginning shire hobbit land unlike english countryside story range across middle earth follow course war ring eye character hobbit frodo baggins samwise sam gamgee meriadoc merry brandybuck peregrin pippin take also hobbit chief ally travel companion men aragorn ranger north boromir captain gondor gimli dwarf warrior legolas greenleaf elven prince gandalf wizard']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_docs = pre_process_documents_parallel(corpus)\n",
    "norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def simple_stemming(text, stemmer=ps):\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDihxH-bZwsU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how do your thought process work when he tell you he be angry'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_lemmatize_text(\"how did your thought process work when he told you he was angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project - Build your Text Pre-processor.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
